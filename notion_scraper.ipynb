{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b70846",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-30T04:27:32.871649Z",
     "start_time": "2022-07-30T04:27:29.771430Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install selenium webdriver-manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc466b69",
   "metadata": {},
   "source": [
    "# Extract data and save to JSON files\n",
    "\n",
    "Save each country (first-level page) in a JSON file, including all topics (second-level page) and their text (third-level page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540ab596",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-02T13:53:59.223155Z",
     "start_time": "2022-08-02T13:45:23.723408Z"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import FirefoxOptions\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "# Create WebDriver and open the initial page\n",
    "opts = FirefoxOptions()\n",
    "opts.add_argument(\"--headless\")\n",
    "browser = webdriver.Firefox(service=Service(GeckoDriverManager().install()), options=opts)\n",
    "browser.set_page_load_timeout(60)\n",
    "browser.get(\"https://uahelpinfo.notion.site/uahelpinfo/UAhelpinfo-70c556bf892748299fe747d95c1b8aa0\")\n",
    "WebDriverWait(browser, 10).until(EC.title_contains(\"UAhelpinfo\"))\n",
    "print(browser.title)\n",
    "\n",
    "# Extract links\n",
    "def get_links(browser):\n",
    "    # Sleep for a random time between 2 and 5\n",
    "    sleep(round(random.uniform(2, 5), 1))\n",
    "    \n",
    "    # Check if links with class=\"pseudoSelection\" are available and extract them\n",
    "    links = WebDriverWait(browser, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.XPATH, '//div[@class=\"pseudoSelection\"]//a')))\n",
    "    return links\n",
    "\n",
    "def get_text(browser):\n",
    "    # Sleep for a random time between 2 and 5\n",
    "    sleep(round(random.uniform(2, 5), 1))\n",
    "    \n",
    "    # Check if links with titles (class=\"Heading 2\") and text are available and extract them\n",
    "    texts_elements = WebDriverWait(browser, 10).until(\n",
    "        EC.presence_of_all_elements_located(\n",
    "            (By.XPATH, '//div[@placeholder=\"Heading 2\" or @data-content-editable-leaf=\"true\"]'))\n",
    "    )\n",
    "    text = []\n",
    "    for text_element in texts_elements:\n",
    "        if text_element.get_attribute('placeholder') == \"Heading 2\":\n",
    "            text.append(\"• \" + text_element.text)\n",
    "        else:\n",
    "            text.append(text_element.text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# Create the output directory if does not exist\n",
    "output_dir = \"output\"\n",
    "if os.path.exists(output_dir) == False:\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# To continue previous scraping, check existing files\n",
    "existing = [file[:-5] for file in os.listdir(output_dir)]\n",
    "\n",
    "# Get country links\n",
    "country_links = get_links(browser)\n",
    "# Extract hrefs\n",
    "links_hrefs = [(country_link.text, country_link.get_attribute('href')) for country_link in country_links]\n",
    "\n",
    "\n",
    "# From each country page, extract topic links\n",
    "href_bar = tqdm(links_hrefs, total=len(links_hrefs))\n",
    "for country, link_href in href_bar:\n",
    "    if country in existing:\n",
    "        continue\n",
    "    else:\n",
    "        href_bar.set_description(country)\n",
    "\n",
    "        country_output = []\n",
    "\n",
    "        # Open the country link\n",
    "        browser.get(link_href)\n",
    "        # Get topic links\n",
    "        topic_links = get_links(browser)\n",
    "        # Extract hrefs\n",
    "        topic_links_hrefs = [(topic_link.text, topic_link.get_attribute('href')) for topic_link in topic_links]\n",
    "\n",
    "        # From each topic page, extract text\n",
    "        for topic, topic_link_href in topic_links_hrefs:\n",
    "            topic_dict = {}\n",
    "\n",
    "            # Open the topic link\n",
    "            browser.get(topic_link_href)\n",
    "            # Extract the text\n",
    "            text = get_text(browser)\n",
    "            # Remove unrequired texgt\n",
    "            text = [p.strip() for p in text if len(p.strip()) > 0 and p.strip().endswith(\"На початок\")==False][:-1]\n",
    "            # Join the list of text portions\n",
    "            text = \"\\n\".join(text)\n",
    "            # Add to the topic dictionary\n",
    "            topic_dict[\"topic\"] = topic\n",
    "            topic_dict[\"text\"] = text\n",
    "            topic_dict[\"url\"] = topic_link_href\n",
    "\n",
    "            country_output.append(topic_dict)\n",
    "\n",
    "        # Save the output to a file\n",
    "        output_path = os.path.join(output_dir, country+\".json\")\n",
    "\n",
    "        with open(output_path, \"w+\") as json_output:\n",
    "            output = json.dumps(country_output, indent=4, ensure_ascii=False)\n",
    "            json_output.write(str(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b51976cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-04T12:08:20.292483Z",
     "start_time": "2022-08-04T12:08:20.257515Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract all Education pages on one JSON\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "output_dir = \"output\"\n",
    "json_files = [file_name for file_name in os.listdir(output_dir) if file_name.endswith(\".json\")]\n",
    "\n",
    "education_items = []\n",
    "\n",
    "for json_file in json_files:\n",
    "    with open(os.path.join(output_dir,json_file)) as json_input:\n",
    "        json_content = json.load(json_input)\n",
    "        for item in json_content:\n",
    "            if item[\"topic\"].startswith(\"Освіта\"):\n",
    "                item[\"Country\"] = json_file[:-5]\n",
    "                education_items.append(item)\n",
    "\n",
    "with open(os.path.join(output_dir,\"Education.json\"), \"w+\") as json_edu_output:\n",
    "    output = json.dumps(education_items, indent=4, ensure_ascii=False)\n",
    "    json_edu_output.write(str(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c65153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
